{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kaggle\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('kmader/food41')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transforms():\n",
    "    train_transforms = T.Compose([T.RandomResizedCrop(224),\n",
    "                                      T.RandomRotation(35),\n",
    "                                      T.RandomVerticalFlip(0.27),\n",
    "                                      T.RandomHorizontalFlip(0.27),\n",
    "                                      T.ToTensor(),\n",
    "                                      T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    valid_n_test_transforms = T.Compose([T.Resize((224,224)),\n",
    "                                       T.ToTensor(),\n",
    "                                       T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    return train_transforms, valid_n_test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading food41.zip to /root/Nutri-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.30G/5.30G [04:25<00:00, 21.5MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = Path('kmader/food41')\n",
    "data_path = Path('data')\n",
    "kaggle.api.dataset_download_cli(str(path))\n",
    "if not data_path.exists():\n",
    "    os.mkdir(data_path)\n",
    "zipfile.ZipFile('food41.zip').extractall(data_path)\n",
    "# with open(data_path/'meta/meta/train.json', 'r') as fp:\n",
    "#     train_dict = json.load(fp)\n",
    "# with open(data_path/'meta/meta/test.json', 'r') as fp:\n",
    "#     test_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_food101_kaggle(batch_size=32, download=True):\n",
    "    \"\"\"\n",
    "    The function to download the Food-101 dataset and split into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Make sure you have your API token file at the correct folder\n",
    "    # Follow https://www.kaggle.com/docs/api#authentication\n",
    "    if download:\n",
    "        path = Path('kmader/food41')\n",
    "        kaggle.api.dataset_download_cli(str(path))\n",
    "        data_path = Path('data')\n",
    "        if not data_path.exists():\n",
    "            os.mkdir(data_path)\n",
    "        zipfile.ZipFile('food41.zip').extractall(data_path)\n",
    "        with open(data_path/'meta/meta/train.json', 'r') as fp:\n",
    "            train_dict = json.load(fp)\n",
    "        with open(data_path/'meta/meta/test.json', 'r') as fp:\n",
    "            test_dict = json.load(fp)\n",
    "        original_data_path = Path('food41/images')\n",
    "        new_folders = ['train', 'test']\n",
    "        for folder in new_folders:\n",
    "            if not os.path.exists(new_data_path/folder):\n",
    "                os.mkdir(new_data_path/folder)\n",
    "            if folder == 'train':\n",
    "                if not os.path.exists(new_data_path/'valid'):\n",
    "                    os.mkdir(new_data_path/'valid')\n",
    "                for key, value in train_dict.items():\n",
    "                    train_value, valid_value = train_test_split(value, train_size=0.75)\n",
    "                    train_set, valid_set = set(train_value), set(valid_value)\n",
    "                    if not os.path.exists(new_data_path/folder/key):\n",
    "                        os.mkdir(new_data_path/folder/key)\n",
    "                    if not os.path.exists(new_data_path/'valid'/key):\n",
    "                        os.mkdir(new_data_path/'valid'/key)\n",
    "                    for image in os.listdir(original_data_path/key):\n",
    "                        image_path = key + '/' + image\n",
    "                        image_path = image_path.split('.')[0]\n",
    "                        if image_path in train_set:\n",
    "                            shutil.copy(original_data_path/key/image, new_data_path/'train'/key/image)\n",
    "                        if image_path in valid_set:\n",
    "                            shutil.copy(original_data_path/key/image, new_data_path/'valid'/key/image)\n",
    "            else:\n",
    "                for key, value in test_dict.items():\n",
    "                    value_set = set(value)\n",
    "                    if not os.path.exists(new_data_path/folder/key):\n",
    "                        os.mkdir(new_data_path/folder/key)\n",
    "                    for image in os.listdir(original_data_path/key):\n",
    "                        image_path = key + '/' + image\n",
    "                        image_path = image_path.split('.')[0]\n",
    "                        if image_path in value_set:\n",
    "                            shutil.copy(original_data_path/key/image, new_data_path/folder/key/image)\n",
    "        shutil.rmtree(original_data_path)\n",
    "    new_data_path = Path('data')\n",
    "    train_dir = new_data_path/'train'\n",
    "    valid_dir = new_data_path/'valid'\n",
    "    test_dir = new_data_path/'test'\n",
    "    train_transforms, valid_n_test_transforms = get_data_transforms()\n",
    "    train_dataset = datasets.ImageFolder(train_dir, transform = train_transforms)\n",
    "    valid_dataset = datasets.ImageFolder(valid_dir, transform = valid_n_test_transforms)\n",
    "    test_dataset = datasets.ImageFolder(test_dir, transform = valid_n_test_transforms)\n",
    "    class_names = test_dataset.classes\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, valid_loader, test_loader, test_dataset, class_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb0494",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
